{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMja0SGhSL8+2lXFB3qlZEg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## 표준 프레임워크\n","- PyTorch와 TensorFlow\n","- 원래는 Keras / Tensorflow / Pytorch 세가지였음\n","- Keras(사용자 친화적)가 Tensorflow에 흡수됨"],"metadata":{"id":"PGic5DQTMpfn"}},{"cell_type":"markdown","source":["### Pytorch - 실행시점에서 그래프 생성 (Define by run)\n","- define by run의 장점\n","    1. 즉시 확인 가능 $\\rightarrow$ pythoinc code\n","    2. GPU 지원, 좋은 API, 커뮤니티\n","    3. 다양한 형태의 딥러닝 함수 지원\n","        - Numpy 구조를 가지는 tensor 객체로 배열 표현\n","        - 자동미분 지원(DL연산)\n","        \n","### Tensorflow - 그래프를 먼저 정의하고 실행시점에 데이터를 넣어줌(Define and run)\n","- define and run의 장점: production과 scalability (생산성과 확장성)"],"metadata":{"id":"ie7eZBolMq5N"}},{"cell_type":"markdown","source":["## **Pytorch Autograd**"],"metadata":{"id":"dJ5qiYLmMt2q"}},{"cell_type":"markdown","source":["### Tensor\n","- 다차원 배열들을 표현하는 pytorch 클래스\n","- numpy.ndarray와 동일\n","- tensor를 생성하는 함수도 거의 동일"],"metadata":{"id":"Pt4YDrE1M4xx"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2TR4hpjMo36","executionInfo":{"status":"ok","timestamp":1678681443737,"user_tz":-540,"elapsed":2918,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"bfbd2e5f-f79f-4cd7-c6c7-868c7e072e5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 2 3 4]\n"," [5 6 7 8 9]]\n","2 (2, 5)\n","tensor([[0., 1., 2., 3., 4.],\n","        [5., 6., 7., 8., 9.]])\n","2 torch.Size([2, 5])\n"]}],"source":["#numpy.ndarray\n","import numpy as np\n","n_array=np.arange(10).reshape(2,5)\n","print(n_array)\n","print(n_array.ndim, n_array.shape)\n","\n","# pytorch -tensor\n","import torch\n","t_array=torch.FloatTensor(n_array)\n","print(t_array)\n","print(t_array.ndim, t_array.shape)"]},{"cell_type":"code","source":["#data to tensor\n","data=[[3,5],[10,5]]\n","print(torch.tensor(data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4A7pRu8M8id","executionInfo":{"status":"ok","timestamp":1678681452632,"user_tz":-540,"elapsed":399,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"30b85676-3770-46ad-887f-ad250fd45a8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 3,  5],\n","        [10,  5]])\n"]}]},{"cell_type":"code","source":["#ndarray to tensor\n","nd_array_ex=np.array(data)\n","print(torch.from_numpy(nd_array_ex))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ikPBpKkM-Bx","executionInfo":{"status":"ok","timestamp":1678681453025,"user_tz":-540,"elapsed":9,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"b88d7c34-18ad-4838-ef3b-138ac999af6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 3,  5],\n","        [10,  5]])\n"]}]},{"cell_type":"code","source":["x_data=torch.tensor([[3,5,20],[10,5,50],[1,5,10]])\n","#pytorch tensor GPU에 올리기\n","if torch.cuda.is_available():\n","  x_data_cuda=x_data.to('cuda')\n","x_data_cuda.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZb4d_YoM-6G","executionInfo":{"status":"ok","timestamp":1678681506027,"user_tz":-540,"elapsed":4282,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"8115be5a-7a1e-426f-90e5-cb22f461e468"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## Tensor handling\n","- view, squeeze, unsqueeze 등으로 tensor 조정\n","  - view: reshape과 동일 (view는 contiguity 보장, reshape은 안해줌) → **view를 써라!!**\n","  - squeeze: 차원의 개수가 1인 차원을 삭제 (압축)\n","  - unsqueeze: 차원의 개수가 1인 차원 추가"],"metadata":{"id":"gJjWWfiqN7xq"}},{"cell_type":"code","source":["tensor_ex=torch.rand(size=(2,3,2))\n","tensor_ex"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3IVSL8LsNc7W","executionInfo":{"status":"ok","timestamp":1678681726506,"user_tz":-540,"elapsed":366,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"6fae7017-c25c-4df1-9bef-fdcd1a114801"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.3317, 0.7010],\n","         [0.3162, 0.1788],\n","         [0.5712, 0.6569]],\n","\n","        [[0.6871, 0.2009],\n","         [0.7494, 0.0994],\n","         [0.2335, 0.5540]]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["tensor_ex.view([-1,6])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3n_AziVOlZO","executionInfo":{"status":"ok","timestamp":1678681744381,"user_tz":-540,"elapsed":386,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"e59b7315-25e5-425a-b62f-fd9362e51eee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3317, 0.7010, 0.3162, 0.1788, 0.5712, 0.6569],\n","        [0.6871, 0.2009, 0.7494, 0.0994, 0.2335, 0.5540]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["tensor_ex.reshape([-1,6]) #view랑 같음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcjvfNAJOoFs","executionInfo":{"status":"ok","timestamp":1678681756727,"user_tz":-540,"elapsed":4,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"fb6f7283-1a18-4e9b-8a70-af720a4e2a45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3317, 0.7010, 0.3162, 0.1788, 0.5712, 0.6569],\n","        [0.6871, 0.2009, 0.7494, 0.0994, 0.2335, 0.5540]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# b가 다름\n","a = torch.zeros(3, 2) \n","b = a.view(2, 3) \n","a.fill_(1) #a를1로 채우면 b도 1임\n","print(b) #contiguity 보장\n","a = torch.zeros(3, 2) \n","b = a.t().reshape(6) # transpose 연산을 거치면서 자료 저장 순서가 원래 방향과 어긋남\n","a.fill_(1) #a를1로 채워도 b는 0임\n","print(b) # transpoe 연산을 거치면서 contiguity가 깨지고, 보장이 깨지는 순간 copy를 하게됨"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urD0HdXmOs6s","executionInfo":{"status":"ok","timestamp":1678683104306,"user_tz":-540,"elapsed":383,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"032c2f00-ae0c-4fc3-838a-daaaa2d7079b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n","tensor([0., 0., 0., 0., 0., 0.])\n"]}]},{"cell_type":"code","source":["tensor_ex=torch.rand(size=(2,1,2))\n","tensor_ex.squeeze() #차원 삭제"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIp3FryQPWPv","executionInfo":{"status":"ok","timestamp":1678682398754,"user_tz":-540,"elapsed":4,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"5e6b782b-130e-45e2-8089-7359d4c4ea1c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3695, 0.4177],\n","        [0.3476, 0.0666]])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["tensor_ex=torch.rand(size=(2,2))\n","print(tensor_ex.unsqueeze(0).shape) #차원 추가\n","print(tensor_ex.unsqueeze(1).shape)\n","print(tensor_ex.unsqueeze(2).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cp5BZ-shPWhV","executionInfo":{"status":"ok","timestamp":1678682474195,"user_tz":-540,"elapsed":4,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"316c7a34-beae-477f-f984-79d2f02fdb12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2, 2])\n","torch.Size([2, 1, 2])\n","torch.Size([2, 2, 1])\n"]}]},{"cell_type":"markdown","source":["## Tensor operations\n","- 기본적인 tensor의 연산은 numpy와 동일\n","- 한가지 다른 것은 **행렬 곱셈** → dot 아닌 mm 사용\n","- matmul (matrix 곱셈)은 broadcasting을 자동 지원"],"metadata":{"id":"6imnu7voRtCg"}},{"cell_type":"code","source":["n1=np.arange(10).reshape(2,5)\n","t1=torch.FloatTensor(n1)\n","n2=np.arange(10).reshape(5,2)\n","t2=torch.FloatTensor(n2)\n","t1.mm(t2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJvEfwr1RcKd","executionInfo":{"status":"ok","timestamp":1678682821760,"user_tz":-540,"elapsed":2,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"d09aa423-d5f2-4c45-95af-c07d32d44086"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 60.,  70.],\n","        [160., 195.]])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["a=torch.rand(5,2,3)\n","b=torch.rand(3)\n","a.matmul(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xOqZVGbYTA9H","executionInfo":{"status":"ok","timestamp":1678682912817,"user_tz":-540,"elapsed":2,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"4cc952e1-3a51-4eb6-cecb-bbbdb0b67339"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.1750, 0.5866],\n","        [0.7481, 0.6300],\n","        [0.3880, 0.6939],\n","        [0.6531, 0.6105],\n","        [0.4382, 0.1671]])"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## Tensor operations for ML/DL formula\n","- nn.functional 모듈을 통해 다양한 수식 변환 지원"],"metadata":{"id":"PL4-a_EFUA7Z"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","tensor= torch.FloatTensor([0.5,0.7,0.1])\n","h_tensor=F.softmax(tensor,dim=0) #softmax\n","h_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQrDt197THR_","executionInfo":{"status":"ok","timestamp":1678683232828,"user_tz":-540,"elapsed":4,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"32416517-0351-4294-ad7c-1ab9f89c86c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.3458, 0.4224, 0.2318])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["y=torch.randint(5,(10,5))\n","y_label=y.argmax(dim=1) #argmax\n","F.one_hot(y_label) #one hot 벡터"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qp6YP_xNUVSa","executionInfo":{"status":"ok","timestamp":1678683302040,"user_tz":-540,"elapsed":8,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"a107d7c4-a506-4473-d9a8-103fca0dd243"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 1, 0, 0],\n","        [0, 1, 0, 0, 0],\n","        [1, 0, 0, 0, 0],\n","        [0, 1, 0, 0, 0],\n","        [0, 0, 0, 1, 0],\n","        [1, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 1],\n","        [1, 0, 0, 0, 0],\n","        [1, 0, 0, 0, 0],\n","        [0, 1, 0, 0, 0]])"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["## **AutoGrad**\n","- Pytorch의 핵심 (자동 미분)\n","- backward 함수 사용"],"metadata":{"id":"ZM_GxtzYUz3Z"}},{"cell_type":"markdown","source":["$$y=w^2$$\n","$$z=10y+25$$\n","$$z=10w^2+25$$"],"metadata":{"id":"OjxQQVByU83B"}},{"cell_type":"code","source":["#w=2일때 미분값 구하기\n","w=torch.tensor(2.0,requires_grad=True) #미분의 대상이 되는 변수를 requires_grad=True로 설정해줌\n","y=w**2\n","z=10*y+25\n","z.backward() #미분\n","w.grad #미분 값"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0ylcDDkUcTg","executionInfo":{"status":"ok","timestamp":1678683504747,"user_tz":-540,"elapsed":4,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"12ce49e9-88be-4acc-9af9-f228dba02b18"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(40.)"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["$$Q=3a^3-b^2$$\n","$$\\frac{\\partial Q}{\\partial a}=9a^2$$\n","$$\\frac{\\partial Q}{\\partial b}=-2b$$"],"metadata":{"id":"pnbRhu3FVtgN"}},{"cell_type":"code","source":["a=torch.tensor([2.,3.],requires_grad=True) #a에 대해 미분 a=[2, 3]\n","b=torch.tensor([6.,4.],requires_grad=True) #b에 대해 미분 b=[6, 4]\n","Q=3*a**3-b**2\n","external_grad=torch.tensor([1.,1.])\n","Q.backward(gradient=external_grad)\n","print(a.grad) #a에 대해 미분\n","print(b.grad) #b에 대해 미분"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGDtHi15VWjo","executionInfo":{"status":"ok","timestamp":1678684198829,"user_tz":-540,"elapsed":3,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"1d821113-3c50-4637-b480-d32eeed0045b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([36., 81.])\n","tensor([-12.,  -8.])\n"]}]},{"cell_type":"markdown","source":["forward pass를 계산하고 PyTorch autograd를 사용하여 그래디언트를 계산하기"],"metadata":{"id":"XWMeREodZQNJ"}},{"cell_type":"code","source":["import torch\n","import math\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n","\n","# Create Tensors to hold input and outputs.\n","# By default, requires_grad=False, which indicates that we do not need to\n","# compute gradients with respect to these Tensors during the backward pass.\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","# Create random Tensors for weights. For a third order polynomial, we need\n","# 4 weights: y = a + b x + c x^2 + d x^3\n","# Setting requires_grad=True indicates that we want to compute gradients with\n","# respect to these Tensors during the backward pass.\n","a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","\n","learning_rate = 1e-6\n","for t in range(2000):\n","    # Forward pass: compute predicted y using operations on Tensors.\n","    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","    # Compute and print loss using operations on Tensors.\n","    # Now loss is a Tensor of shape (1,)\n","    # loss.item() gets the scalar value held in the loss.\n","    loss = (y_pred - y).pow(2).sum()\n","    if t % 100 == 99:\n","        print(t, loss.item())\n","\n","    # Use autograd to compute the backward pass. This call will compute the\n","    # gradient of loss with respect to all Tensors with requires_grad=True.\n","    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n","    # the gradient of the loss with respect to a, b, c, d respectively.\n","    loss.backward()\n","\n","    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n","    # because weights have requires_grad=True, but we don't need to track this\n","    # in autograd.\n","    with torch.no_grad():\n","        a -= learning_rate * a.grad\n","        b -= learning_rate * b.grad\n","        c -= learning_rate * c.grad\n","        d -= learning_rate * d.grad\n","\n","        # Manually zero the gradients after updating weights\n","        a.grad = None\n","        b.grad = None\n","        c.grad = None\n","        d.grad = None\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXKVPtjSY21i","executionInfo":{"status":"ok","timestamp":1678684421718,"user_tz":-540,"elapsed":552,"user":{"displayName":"이윤표","userId":"15778647441739081017"}},"outputId":"1e50f93e-66de-4894-9db0-5aedefd6eb1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["99 339.5775451660156\n","199 242.1921844482422\n","299 173.51095581054688\n","399 125.06243133544922\n","499 90.87954711914062\n","599 66.75736999511719\n","699 49.731815338134766\n","799 37.713050842285156\n","899 29.227426528930664\n","999 23.235429763793945\n","1099 19.003686904907227\n","1199 16.014694213867188\n","1299 13.903253555297852\n","1399 12.411544799804688\n","1499 11.357565879821777\n","1599 10.612792015075684\n","1699 10.086457252502441\n","1799 9.714466094970703\n","1899 9.4515380859375\n","1999 9.265680313110352\n","Result: y = 0.022273151203989983 + 0.8590679168701172 x + -0.003842489328235388 x^2 + -0.09366139769554138 x^3\n"]}]},{"cell_type":"markdown","source":["### 새롭게 알게된점\n","- view와 reshape의 차이점 contiguity 관점에서 차이가 나며 reshape은 contiguity를 보장해주지 않으므로 view 사용을 권장한다는 것!\n","- unsqueeze와 squeeze의 차이점\n","- tensor에서는 행렬곱셈에 dot이 아닌 mm을 쓴다는 것\n","- matmul은 broadcasting을 지원한다는 것\n","- nn.functional 모듈의 다양한 함수\n","- autograd 과정"],"metadata":{"id":"V815vx_mZZe_"}}]}